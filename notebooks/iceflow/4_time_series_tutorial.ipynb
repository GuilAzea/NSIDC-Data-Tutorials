{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# *IceFlow & icepyx*: Altimetry Time Series Tutorial\n",
    "### NASA Earthdata Webinar - April 2021</b>\n",
    "\n",
    "This tutorial demonstrates how to harmonize several NASA altimetry data sets with varying temporal coverage, formats, and coordinate reference frames using the IceFlow and icepyx Python tools. Please refer to the 0_introduction.ipynb notebook for detailed information on the data sets you will be exploring in this tutorial. \n",
    "\n",
    "#### Objectives:\n",
    "1. Use the IceFlow map widget to select and visualize an area of interest.\n",
    "2. Access coincident ICESat/Glas, Operation IceBridge, and ICESat-2 data over the same spatial region.\n",
    "3. Use the community-developed icepyx python library to subset ICESat-2 data.\n",
    "4. Learn about advanced icepyx capabilities including data value visualization prior to download. \n",
    "5. Extract common data variables into a Geopandas dataframe.\n",
    "7. Plot and visualize the altimetry time series to detect glacial elevation change over time.\n",
    "\n",
    "<b>Authors:</b><br />\n",
    "<span style=\"font-size:larger;\">Jessica Scheick</span>, *University of New Hampshire*, Durham, New Hampshire<br />\n",
    "<span style=\"font-size:larger;\">Nicholas Kotlinski & Amy Steiker</span>, *NASA National Snow and Ice Data Center DAAC*, Boulder, Colorado, USA\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Running this tutorial locally\n",
    "\n",
    "To run this notebook locally, you must first set up your computing environment. Please see the [repository readme](https://github.com/nsidc/NSIDC-Data-Tutorials#usage-with-binder) for instructions on several ways (using Binder, Docker, or Conda) to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1. NASA's Earthdata Credentials\n",
    "\n",
    "To access data using the *IceFlow* library and *icepyx* package, it is necessary to log into [Earthdata Login](https://urs.earthdata.nasa.gov/). To do this, enter your NASA Earthdata credentials in the next step after executing the following code cell.\n",
    "\n",
    "**Note**: If you don't have NASA Earthdata credentials you will need to register first at the link above. An account is free and available to everyone!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell will prompt you for your Earthdata username and password. Press\n",
    "from iceflow.ui import IceFlowUI\n",
    "client = IceFlowUI()\n",
    "client.display_credentials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# This cell will verify if your credentials are valid. \n",
    "# This may take a little while. If it fails, please try again.\n",
    "\n",
    "authorized = client.authenticate()\n",
    "if authorized is None:\n",
    "    print('Earthdata Login not successful')\n",
    "else:\n",
    "    print('Earthdata Login successful!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note:** If the output shows \"You are logged into NASA Earthdata!\", then you are ready to proceed!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accessing and harmonizing data across missions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Available data\n",
    "\n",
    "|IceFlow Name | Data Set| Spatial Coverage | Temporal Coverage| Mission  | Sensors  |\n",
    "|-------------|---------|------------------|------------------|----------|----------|\n",
    "**ATM1B** |[BLATM L1B](https://nsidc.org/data/BLATM1B)| South: N:-53, S: -90, E:180, W:-180 <br> North: N:90, S: 60, E:180, W:-180 | 23 Jun. 1993 - 30 Oct. 2008 | Pre-IceBridge | ATM  | \n",
    "**ATM1B** |[ILATM L1B V1](https://nsidc.org/data/ILATM1B/versions/1) | South: N:-53, S: -90, E:180, W:-180 <br> North: N:90, S: 60, E:180, W:-180 | 31 Mar. 2009 - 8 Nov. 2012  <br> (updated 2013) | IceBridge | ATM | \n",
    "**ATM1B** |[ILATM L1B V2](https://nsidc.org/data/ILATM1B/versions/2)| South: N:-53, S: -90, E:180, W:-180 <br> North: N:90, S: 60, E:180, W:-180 | 20 Mar. 2013 - 16 May 2019  <br> (updated 2020)| IceBridge|ATM|\n",
    "**ILVIS2** |[ILVIS2](https://nsidc.org/data/ILVIS2)| North: N:90, S: 60, E:180, W:-180|25 Aug. 2017 - 20 Sept. 2017|IceBridge | ALTIMETERS, LASERS, LVIS |\n",
    "**GLAH06** |[GLAH06](https://nsidc.org/data/GLAH06/)| Global: N:86, S: -86, E:180, W:-180|20 Feb. 2003 - 11 Oct. 2009|ICESat/GLAS | ALTIMETERS, CD, GLAS, GPS, <br> GPS Receiver, LA, PC|\n",
    "\n",
    "**Notes**:\n",
    "* Due to the nature of the **ILVIS2** product, IceFlow doesn't provide a common dictionary. Data is accessible, but the user will need to harmonize the data to their own specifications.\n",
    "* If you have questions about the data sets please refer to the user guides or contact NSIDC user services at nsidc@nsidc.org\n",
    "\n",
    "--- \n",
    "\n",
    "\n",
    "#### 2.2. Choosing Corrections: Using the ITRF and Epoch values\n",
    "* The differences between ITRF corrections is negligible in most cases, and corrections should only be applied by users who are familiar with the procedures behind these corrections.\n",
    "\n",
    "* The optional ***ITRF*** parameter allows you to choose an ITRF reference to which the data will be transformed via the published ITRF transformation parameters. This parameter is optional but must be used if you want to specify an epoch. Available values are: **ITRF2000, ITRF2008, ITRF2014**</br>\n",
    "Example: `'ITRF': '2014'`\n",
    "* The ***epoch*** parameter is optional and entered in decimal years to which the data will be transformed via the ITRF Plate Motion Model corresponding to ITRF. This parameter can only be used if the ***ITRF*** parameter is specified and set to either 2008 or 20014, as only ITRF2008 and ITRF2014 have a plate motion model. </br>\n",
    "Example: `'epoch': '2018.1'` (This specifies January 2018.)\n",
    "\n",
    "ICESat-2: `ITRF2014`\n",
    "\n",
    "ICESat/Glas: `ITRF2008`\n",
    "  \n",
    "IceBridge/Pre-IceBridge ILATM1B: `ITRF2008`\n",
    "\n",
    "IceBridge ILVIS2: `ITRF2000`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "#### 2.3. Accessing Data with the *IceFlow* Access Widget\n",
    "The *IceFlow* access widget is a user interface tool to visualize flightpaths from IceBridge, draw a region of interest, set spatio-temporal parameters and place data orders to the *IceFlow* API and *icepyx* package without the need to write code.\n",
    "The output of the operations performed in the widget can be seen in the log window (right-most icon at the bottom of your browser.) \n",
    "<img src='./img/log-icons.png'> or by selecting it on the _View_ menu \"Show log console\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Let's start with the user interface. Using 'horizontal' will add the widget inline.\n",
    "client.display_map('horizontal', extra_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "#### 2.4. Accessing data with the *IceFlow* API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Small example subset over Sermeq Kujalleq (Jakobshavn Isbrae):\n",
    "my_params1 ={\n",
    "    'datasets': ['GLAH06', 'ATL06'],\n",
    "    'start': '2007-01-01',\n",
    "    'end': '2018-12-31',\n",
    "    'bbox': '-49.6,69.1,-49.3,69.17',\n",
    "\n",
    "    # Here we will select ITRF2014 to match ICESat-2 an Epoch of the most recent ICESat-2 granule we are ordering\n",
    "    'itrf': 'ITRF2014',\n",
    "    'epoch': '2018.12'\n",
    "}\n",
    "\n",
    "# returns a json dictionary, the request parameters and the order's response.\n",
    "granules_metadata = client.query_cmr(params=my_params1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since the IceBridge data is so dense, we order a smaller subset to decrease order and download times\n",
    "my_params2 ={\n",
    "    'datasets': ['ATM1B'],\n",
    "    'start': '2007-01-01',\n",
    "    'end': '2018-12-31',\n",
    "    'bbox': '-49.53,69.12,-49.51,69.135',\n",
    "    # Here we will select ITRF2014 to match ICESat-2 an Epoch of the most recent ICESat-2 granule we are ordering\n",
    "    'itrf': 'ITRF2014',\n",
    "    'epoch': '2018.12'\n",
    "}\n",
    "\n",
    "# returns a json dictionary, the request parameters and the order's response.\n",
    "granules_metadata = client.query_cmr(params=my_params2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "orders1 = client.place_data_orders(params=my_params1)\n",
    "print(orders1)\n",
    "orders2 = client.place_data_orders(params=my_params2)\n",
    "print(orders2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Check Order Status\n",
    "The following cell will show you the status of your data order. You can proceed in the notebook once all orders are \"COMPLETE\". If you proceed earlier only the completed data orders will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for order in orders1:\n",
    "    status = client.order_status(order)\n",
    "    print(order['dataset'], order['id'], status['status'])\n",
    "    \n",
    "for order in orders2:\n",
    "    status = client.order_status(order)\n",
    "    print(order['dataset'], order['id'], status['status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Download Data\n",
    "Once all data orders are \"COMPLETE\", you can proceed downloading the data. The data are downloaded to the /data folder of this notebook directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for order in orders1:\n",
    "    status = client.order_status(order)\n",
    "    if status['status'] == 'COMPLETE':\n",
    "        client.download_order(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order in orders2:\n",
    "    status = client.order_status(order)\n",
    "    if status['status'] == 'COMPLETE':\n",
    "        client.download_order(order)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---\n",
    "#### 2.5. Downloading ICESat-2 data [directly] with ***icepyx***\n",
    "Behind the scenes, *IceFlow* is using the [*icepyx*](https://icepyx.readthedocs.io/en/latest/) Python package to download ICESat-2 data. *icepyx* is a standalone library that includes its own examples and documentation and welcomes contributions from data users (no previous GitHub or software development experience required!). Thus, it has a lot of additional functionality for querying, subsetting, ordering, and downloading ICESat-2 datasets (with in-the-works additions for data ingest into multiple formats), including making it easier to programmatically download data from multiple regions. Here we highlight some of the data visualization capabilities for exploring data prior to order and download."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the icepyx query object and import icepyx\n",
    "# Note: if you would like to order additional ICESat-2 data using icepyx, you'll need to attach an Earthdata\n",
    "# session to your icepyx query object (or re-login to Earthdata). See [icepyx examples](https://icepyx.readthedocs.io/en/latest/getting_started/example_link.html) for more details.\n",
    "import icepyx as ipx\n",
    "bbox_list = [float(val) for val in (my_params1[\"bbox\"].split(\",\"))]\n",
    "is2_obj = ipx.Query(str(my_params1[\"datasets\"][-1]), bbox_list, [my_params1[\"start\"], my_params1[\"end\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the query extent (this map won't be interactive if you don't have geoviews and the dev version of icepyx installed)\n",
    "# Thus, for very small areas it can be difficult to see the specified region on a static world map (an area for future development!)\n",
    "is2_obj.visualize_spatial_extent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 3. Working with the data\n",
    "Now that we have downloaded our data, we need to make sure that they are in a common format to do analysis across missions.\n",
    "\n",
    "Although typically we would include all import statements at the start of the workflow, here we have separated them into this section for instructional clarity.\n",
    "\n",
    "The main Python packages/libraries that will be used in this notebook are:\n",
    "\n",
    "* [*cartopy*](https://scitools.org.uk/cartopy/docs/latest/):\n",
    "A Python package designed for geospatial data processing in order to produce maps and other geospatial data analyses.\n",
    "* [*geopandas*](https://geopandas.org/): \n",
    "Library to simplify working with geospatial data in Python (using pandas).\n",
    "* [*h5py*](https://github.com/h5py/h5py):\n",
    "Pythonic wrapper around the [*HDF5 library](https://en.wikipedia.org/wiki/Hierarchical_Data_Format) \n",
    "* [*matplotlib*](https://matplotlib.org/):\n",
    "Comprehensive library for creating static, animated, and interactive visualizations in Python\n",
    "* [*vaex*](https://github.com/vaexio/vaex):\n",
    "High performance Python library for lazy Out-of-Core dataframes (similar to *pandas*), to visualize and explore big tabular data sets\n",
    "* [*pandas*](https://pandas.pydata.org/):\n",
    "Open source data analysis and manipulation tool\n",
    "* [*icepyx*](https://icepyx.readthedocs.io/en/latest/):\n",
    "Library for ICESat-2 data users\n",
    " \n",
    "**Note**: *Warnings* are being ignored to suppress verbose warnings from some libraries (i.e. vaex, h5py). This will not prevent users from seeing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs #geospatial (mapping) plotting library\n",
    "import cartopy.io.img_tiles as cimgt\n",
    "import geopandas as gpd #add geospatial awareness/functionality to pandas\n",
    "import h5py\n",
    "from iceflow.processing import IceFlowProcessing as ifp\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt #Python visualization\n",
    "import vaex\n",
    "import pandas as pd #data analysis and manipulation tool\n",
    "import numpy as np\n",
    "import warnings #Python warnings module\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1. Import data and convert to a geopandas data frame\n",
    "ICESat, ICESat-2 and IceBridge data can be read in using preconfigured common dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# ICESat granule data\n",
    "glas_gdf = ifp.to_geopandas('data/GLAH06-20210423-Sample.h5')\n",
    "glas_gdf['mission'] = \"IS\"\n",
    "\n",
    "# Pre-IceBridge/IceBridge ATM granule data\n",
    "#preib_gdf = ifp.to_geopandas('data/ATM1B-20210423-Sample.h5')\n",
    "#preib_gdf['mission'] = \"IB\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICESat-2 granule data\n",
    "is2_gdf = ifp.to_geopandas('data/ATL06_20190201020557_05290204_004_01.h5')\n",
    "is2_gdf['mission'] = \"IS2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>geometry</th>\n",
       "      <th>mission</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2007-03-22 19:50:29.545575</th>\n",
       "      <td>69.168590</td>\n",
       "      <td>-49.326690</td>\n",
       "      <td>644.330</td>\n",
       "      <td>POINT (-49.32669 69.16859)</td>\n",
       "      <td>IS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-22 19:50:29.570575</th>\n",
       "      <td>69.167068</td>\n",
       "      <td>-49.327619</td>\n",
       "      <td>648.844</td>\n",
       "      <td>POINT (-49.32762 69.16707)</td>\n",
       "      <td>IS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-22 19:50:29.595575</th>\n",
       "      <td>69.165547</td>\n",
       "      <td>-49.328545</td>\n",
       "      <td>651.840</td>\n",
       "      <td>POINT (-49.32855 69.16555)</td>\n",
       "      <td>IS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-22 19:50:29.620575</th>\n",
       "      <td>69.164027</td>\n",
       "      <td>-49.329470</td>\n",
       "      <td>654.344</td>\n",
       "      <td>POINT (-49.32947 69.16403)</td>\n",
       "      <td>IS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2007-03-22 19:50:29.645575</th>\n",
       "      <td>69.162507</td>\n",
       "      <td>-49.330395</td>\n",
       "      <td>661.877</td>\n",
       "      <td>POINT (-49.33040 69.16251)</td>\n",
       "      <td>IS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             latitude  longitude  elevation  \\\n",
       "time                                                          \n",
       "2007-03-22 19:50:29.545575  69.168590 -49.326690    644.330   \n",
       "2007-03-22 19:50:29.570575  69.167068 -49.327619    648.844   \n",
       "2007-03-22 19:50:29.595575  69.165547 -49.328545    651.840   \n",
       "2007-03-22 19:50:29.620575  69.164027 -49.329470    654.344   \n",
       "2007-03-22 19:50:29.645575  69.162507 -49.330395    661.877   \n",
       "\n",
       "                                              geometry mission  \n",
       "time                                                            \n",
       "2007-03-22 19:50:29.545575  POINT (-49.32669 69.16859)      IS  \n",
       "2007-03-22 19:50:29.570575  POINT (-49.32762 69.16707)      IS  \n",
       "2007-03-22 19:50:29.595575  POINT (-49.32855 69.16555)      IS  \n",
       "2007-03-22 19:50:29.620575  POINT (-49.32947 69.16403)      IS  \n",
       "2007-03-22 19:50:29.645575  POINT (-49.33040 69.16251)      IS  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(212, 5)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Let's see what's in the harmonized dataframe and its shape.\n",
    "display(glas_gdf.head(), glas_gdf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# and again for ICESat-2 ATL06\n",
    "display(is2_gdf.head(), is2_gdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2. Down sample IceBridge Data\n",
    "Due to the size of the IceBridge ATM1B point cloud, it is often difficult to work with or plot data in a Notebook environment. We will downsample the data in this example for faster plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                                    </th><th>latitude  </th><th>longitude  </th><th>elevation   </th><th>date                         </th><th>index    </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;0&lt;/i&gt;        </td><td>69.127761 </td><td>-49.493261 </td><td>-1816.333008</td><td>2007-09-20 16:38:11.122000000</td><td>0.0      </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;1&lt;/i&gt;        </td><td>69.129082 </td><td>-49.493192 </td><td>-1671.847046</td><td>2007-09-20 16:38:11.394000000</td><td>1.0      </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;2&lt;/i&gt;        </td><td>69.130029 </td><td>-49.541076 </td><td>232.735001  </td><td>2008-06-27 16:52:40.187000000</td><td>2.0      </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3&lt;/i&gt;        </td><td>69.130081 </td><td>-49.540974 </td><td>233.444     </td><td>2008-06-27 16:52:40.187000000</td><td>3.0      </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;4&lt;/i&gt;        </td><td>69.130265 </td><td>-49.540582 </td><td>236.263     </td><td>2008-06-27 16:52:40.240000000</td><td>4.0      </td></tr>\n",
       "<tr><td>...                                  </td><td>...       </td><td>...        </td><td>...         </td><td>...                          </td><td>...      </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3,385,808&lt;/i&gt;</td><td>69.120001 </td><td>-49.52292  </td><td>257.800995  </td><td>2018-04-30 15:00:36.663000000</td><td>3385808.0</td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3,385,809&lt;/i&gt;</td><td>69.11999  </td><td>-49.522447 </td><td>258.140991  </td><td>2018-04-30 15:00:36.664000000</td><td>3385809.0</td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3,385,810&lt;/i&gt;</td><td>69.11999  </td><td>-49.522402 </td><td>256.713013  </td><td>2018-04-30 15:00:36.665000000</td><td>3385810.0</td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3,385,811&lt;/i&gt;</td><td>69.119989 </td><td>-49.522359 </td><td>255.509995  </td><td>2018-04-30 15:00:36.665000000</td><td>3385811.0</td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3,385,812&lt;/i&gt;</td><td>69.119987 </td><td>-49.522316 </td><td>255.248001  </td><td>2018-04-30 15:00:36.665000000</td><td>3385812.0</td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "#          latitude    longitude    elevation     date                           index\n",
       "0          69.127761   -49.493261   -1816.333008  2007-09-20 16:38:11.122000000  0.0\n",
       "1          69.129082   -49.493192   -1671.847046  2007-09-20 16:38:11.394000000  1.0\n",
       "2          69.130029   -49.541076   232.735001    2008-06-27 16:52:40.187000000  2.0\n",
       "3          69.130081   -49.540974   233.444       2008-06-27 16:52:40.187000000  3.0\n",
       "4          69.130265   -49.540582   236.263       2008-06-27 16:52:40.240000000  4.0\n",
       "...        ...         ...          ...           ...                            ...\n",
       "3,385,808  69.120001   -49.52292    257.800995    2018-04-30 15:00:36.663000000  3385808.0\n",
       "3,385,809  69.11999    -49.522447   258.140991    2018-04-30 15:00:36.664000000  3385809.0\n",
       "3,385,810  69.11999    -49.522402   256.713013    2018-04-30 15:00:36.665000000  3385810.0\n",
       "3,385,811  69.119989   -49.522359   255.509995    2018-04-30 15:00:36.665000000  3385811.0\n",
       "3,385,812  69.119987   -49.522316   255.248001    2018-04-30 15:00:36.665000000  3385812.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in the data and common dictionary\n",
    "filepath = 'data/ATM1B-20210423-Sample.h5'\n",
    "atm_key = ifp.get_common_dictionary('ATM')\n",
    "\n",
    "f = h5py.File(filepath, 'r')\n",
    "preib_vx = vaex.open(filepath)\n",
    "\n",
    "preib_vx['date'] = preib_vx.utc_datetime.values.astype('datetime64[ns]')\n",
    "preib_df = atm_vx[atm_key['latitude'], atm_key['longitude'], atm_key['elevation'], 'date']\n",
    "preib_df.add_column('index', vaex.vrange(0, len(preib_vx)))\n",
    "display(preib_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead>\n",
       "<tr><th>#                                 </th><th>latitude  </th><th>longitude  </th><th>elevation   </th><th>date                         </th><th>index    </th><th>mission  </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;0&lt;/i&gt;     </td><td>69.127761 </td><td>-49.493261 </td><td>-1816.333008</td><td>2007-09-20 16:38:11.122000000</td><td>0.0      </td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;1&lt;/i&gt;     </td><td>69.130141 </td><td>-49.540565 </td><td>240.117004  </td><td>2008-06-27 16:52:40.509000000</td><td>100.0    </td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;2&lt;/i&gt;     </td><td>69.129584 </td><td>-49.541695 </td><td>238.651993  </td><td>2008-06-27 16:52:40.673000000</td><td>200.0    </td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;3&lt;/i&gt;     </td><td>69.129798 </td><td>-49.541006 </td><td>244.223999  </td><td>2008-06-27 16:52:40.833000000</td><td>300.0    </td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;4&lt;/i&gt;     </td><td>69.129254 </td><td>-49.542389 </td><td>240.735992  </td><td>2008-06-27 16:52:40.943000000</td><td>400.0    </td><td>IB       </td></tr>\n",
       "<tr><td>...                               </td><td>...       </td><td>...        </td><td>...         </td><td>...                          </td><td>...      </td><td>...      </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;33,854&lt;/i&gt;</td><td>69.120412 </td><td>-49.523633 </td><td>247.108002  </td><td>2018-04-30 15:00:36.251000000</td><td>3385400.0</td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;33,855&lt;/i&gt;</td><td>69.119995 </td><td>-49.525912 </td><td>247.442001  </td><td>2018-04-30 15:00:36.327000000</td><td>3385500.0</td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;33,856&lt;/i&gt;</td><td>69.120157 </td><td>-49.521689 </td><td>246.391006  </td><td>2018-04-30 15:00:36.379000000</td><td>3385600.0</td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;33,857&lt;/i&gt;</td><td>69.120001 </td><td>-49.521329 </td><td>250.070007  </td><td>2018-04-30 15:00:36.462000000</td><td>3385700.0</td><td>IB       </td></tr>\n",
       "<tr><td>&lt;i style=&#x27;opacity: 0.6&#x27;&gt;33,858&lt;/i&gt;</td><td>69.120027 </td><td>-49.522455 </td><td>258.973999  </td><td>2018-04-30 15:00:36.623000000</td><td>3385800.0</td><td>IB       </td></tr>\n",
       "</tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "#       latitude    longitude    elevation     date                           index      mission\n",
       "0       69.127761   -49.493261   -1816.333008  2007-09-20 16:38:11.122000000  0.0        IB\n",
       "1       69.130141   -49.540565   240.117004    2008-06-27 16:52:40.509000000  100.0      IB\n",
       "2       69.129584   -49.541695   238.651993    2008-06-27 16:52:40.673000000  200.0      IB\n",
       "3       69.129798   -49.541006   244.223999    2008-06-27 16:52:40.833000000  300.0      IB\n",
       "4       69.129254   -49.542389   240.735992    2008-06-27 16:52:40.943000000  400.0      IB\n",
       "...     ...         ...          ...           ...                            ...        ...\n",
       "33,854  69.120412   -49.523633   247.108002    2018-04-30 15:00:36.251000000  3385400.0  IB\n",
       "33,855  69.119995   -49.525912   247.442001    2018-04-30 15:00:36.327000000  3385500.0  IB\n",
       "33,856  69.120157   -49.521689   246.391006    2018-04-30 15:00:36.379000000  3385600.0  IB\n",
       "33,857  69.120001   -49.521329   250.070007    2018-04-30 15:00:36.462000000  3385700.0  IB\n",
       "33,858  69.120027   -49.522455   258.973999    2018-04-30 15:00:36.623000000  3385800.0  IB"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Here we will aggrigate or \"decimate\" the data to make it smaller for our purposes\n",
    "preib_dec = preib_df[(preib_df.index % 100 == 0)]\n",
    "ib = np.array([\"IB\"]*len(preib_df))\n",
    "preib_dec.add_column('mission', ib)\n",
    "display(preib_dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>elevation</th>\n",
       "      <th>date</th>\n",
       "      <th>mission</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>69.127761</td>\n",
       "      <td>-49.493261</td>\n",
       "      <td>-1816.333008</td>\n",
       "      <td>2007-09-20 16:38:11.122</td>\n",
       "      <td>IB</td>\n",
       "      <td>POINT (-49.49326 69.12776)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>69.130141</td>\n",
       "      <td>-49.540565</td>\n",
       "      <td>240.117004</td>\n",
       "      <td>2008-06-27 16:52:40.509</td>\n",
       "      <td>IB</td>\n",
       "      <td>POINT (-49.54057 69.13014)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>69.129584</td>\n",
       "      <td>-49.541695</td>\n",
       "      <td>238.651993</td>\n",
       "      <td>2008-06-27 16:52:40.673</td>\n",
       "      <td>IB</td>\n",
       "      <td>POINT (-49.54169 69.12958)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>69.129798</td>\n",
       "      <td>-49.541006</td>\n",
       "      <td>244.223999</td>\n",
       "      <td>2008-06-27 16:52:40.833</td>\n",
       "      <td>IB</td>\n",
       "      <td>POINT (-49.54101 69.12980)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69.129254</td>\n",
       "      <td>-49.542389</td>\n",
       "      <td>240.735992</td>\n",
       "      <td>2008-06-27 16:52:40.943</td>\n",
       "      <td>IB</td>\n",
       "      <td>POINT (-49.54239 69.12925)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    latitude  longitude    elevation                    date mission  \\\n",
       "0  69.127761 -49.493261 -1816.333008 2007-09-20 16:38:11.122      IB   \n",
       "1  69.130141 -49.540565   240.117004 2008-06-27 16:52:40.509      IB   \n",
       "2  69.129584 -49.541695   238.651993 2008-06-27 16:52:40.673      IB   \n",
       "3  69.129798 -49.541006   244.223999 2008-06-27 16:52:40.833      IB   \n",
       "4  69.129254 -49.542389   240.735992 2008-06-27 16:52:40.943      IB   \n",
       "\n",
       "                     geometry  \n",
       "0  POINT (-49.49326 69.12776)  \n",
       "1  POINT (-49.54057 69.13014)  \n",
       "2  POINT (-49.54169 69.12958)  \n",
       "3  POINT (-49.54101 69.12980)  \n",
       "4  POINT (-49.54239 69.12925)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(33859, 6)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we need to convert our downsampled data back into a pandas geodataframe so we can merge it with the other missions\n",
    "preib_pandas = preib_dec.to_pandas_df([\"latitude\",\"longitude\", \"elevation\", \"date\", \"mission\"])\n",
    "\n",
    "preib_gdf = gpd.GeoDataFrame(preib_pandas,\n",
    "                                geometry=gpd.points_from_xy(preib_pandas['longitude'],\n",
    "                                                            preib_pandas['latitude'],\n",
    "                                                            crs='epsg:4326'))\n",
    "display(preib_gdf.head(), preib_gdf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3. Plot the data from each mission together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's plot all three datasets together, using color to show elevation\n",
    "# Note that although this data is projected, it is not recommended you use this map as a basis for geospatial analysis\n",
    "\n",
    "# Create a Stamen terrain background instance.\n",
    "stamen_terrain = cimgt.Stamen('terrain-background')\n",
    "\n",
    "map_fig = plt.figure()\n",
    "# Create a GeoAxes in the tile's projection.\n",
    "map_ax = map_fig.add_subplot(111, projection=stamen_terrain.crs)\n",
    "\n",
    "# Limit the extent of the map to a small longitude/latitude range.\n",
    "map_ax.set_extent([-56, -45, 67, 71], crs=ccrs.Geodetic())\n",
    "\n",
    "# Add the Stamen data at zoom level 8.\n",
    "map_ax.add_image(stamen_terrain, 8)\n",
    "\n",
    "# world = gpd.read_file(gpd.datasets.get_path(\"naturalearth_lowres\"))\n",
    "# world.plot(ax=map_ax, facecolor=\"lightgray\", edgecolor=\"gray\")\n",
    "\n",
    "# ####ATTN: Need to get the correct projection in here!\n",
    "for onegdf, lab, shp in zip([preib_gdf, glas_gdf],[\"preib\",\"glas\"], ['P','o','D']):\n",
    "    ms=map_ax.scatter(onegdf[\"longitude\"], onegdf[\"latitude\"],  2, c=onegdf[\"elevation\"],\n",
    "                      vmin=0, vmax=1000, label=lab, marker=shp,\n",
    "                      transform=ccrs.Geodetic())\n",
    "\n",
    "plt.colorbar(ms, label='elevation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4. Stack the dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to the harmonization, we can stack our geopandas dataframes to have a unified dataframe for analysis.\n",
    "stacked_df = gpd.GeoDataFrame(pd.concat([preib_gdf, glas_gdf]))\n",
    "display(stacked_df.head(), stacked_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.5. Time series analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We zoom in on a very small section of our data to plot a time series\n",
    "tsgdf_full = stacked_df.loc[(stacked_df[\"longitude\"]>=-49.526) & (stacked_df[\"longitude\"]<=-49.521)\n",
    "              & (stacked_df[\"latitude\"]>=69.121) & (stacked_df[\"latitude\"]<=69.125) ]\n",
    "# filter out erroneous elevation values\n",
    "tsgdf_full = tsgdf_full.loc[tsgdf_full[\"elevation\"] > 0]\n",
    "\n",
    "print(len(tsgdf_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to plot as a time series, we cannot have duplicate x (time) values. Since the data collection rates\n",
    "# are on the order of seconds, we keep the average where there are multiple records per second\n",
    "tsgdf = tsgdf_full.groupby('time').mean()\n",
    "\n",
    "# we also need to make \"time\" a non-index column\n",
    "tsgdf[\"time_col\"] = tsgdf.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsgdf.plot(x=\"time_col\",y=\"elevation\", kind=\"scatter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Use code and example from https://nbviewer.jupyter.org/github/nicholas-kotlinski/2020_ICESat-2_Hackweek_Tutorials/blob/master/05.Geospatial_Analysis/shean_ICESat-2_hackweek_tutorial_GeospatialAnalysis_rendered.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## [Draft notebook for April 2021 Earthdata Webinar]\n",
    "\n",
    "#### Learning Objectives (tbd)\n",
    "* (goal from discussion) Introduce audience to the “Ice cubed” (ICESat/OIB/ICESat-2) missions and products\n",
    "* (goal from discussion Promote icepyx and IceFlow as a way to address the need to access/harmonize multiple data products/resolutions/formats/structure\n",
    "\n",
    "\n",
    "#### Webinar Outline\n",
    "* Intro of harmonization challenge\n",
    "    - Moving one level deeper into the need for time series analysis across disparate platforms\n",
    "* Intro to the altimetry missions\n",
    "    - Use the IceFlow intro notebook to guide this\n",
    "    - Highlight the impressive time series\n",
    "* Overview of IceFlow and icepyx\n",
    "    - Audience and use cases of these tools\n",
    "    - Icepyx genesis, open science and community development principles\n",
    "        - working on that balance between community development and end use\n",
    "    - IceFlow overview\n",
    "    - How do these tools address challenges\n",
    "        - IceFlow aims to make it easier to harmonize OIB/ICESat data with ICESat-2 using backend IceFlow API\n",
    "* Use Case / Application\n",
    "    - Leverage ICESat-2 hackweek topics\n",
    "    - https://github.com/ICESAT-2HackWeek/geospatial-analysis/blob/master/shean_ICESat-2_hackweek_tutorial_GeospatialAnalysis.ipynb\n",
    "* Short navigation of nsidc site, how to get to tools, notebook\n",
    "    - Have Jennifer post links while we talk.\n",
    "* Demos\n",
    "    - **Use this notebook to pulls pieces of iceflow / icepyx using a specific science application**"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
