{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src='./img/nsidc_logo.png'/>\n",
    "\n",
    "# **IceFlow**\n",
    "### Point Cloud Data Access\n",
    "</center>\n",
    "\n",
    "---\n",
    "<div>\n",
    "<img align=\"right\" width=\"50%\" height=\"200px\" src='./img/vaex.png'/>\n",
    "</div>\n",
    "\n",
    "## Goals of this notebook\n",
    "This Jupyter notebook is an **interactive document** to teach students, researchers and others interested in cryospheric sciences how to access and work with **airborne altimetry and related data sets from NASAâ€™s [IceBridge](https://www.nasa.gov/mission_pages/icebridge/index.html) mission, and satellite altimetry data from [ICESat](https://icesat.gsfc.nasa.gov/icesat/) and [ICESat-2](https://icesat-2.gsfc.nasa.gov/) missions** using the NSIDC **IceFlow API**.\n",
    "\n",
    "> If you want to know what an API is, take a look at this video: [\"What is an API?\"](https://www.youtube.com/watch?v=s7wmiS2mSXY) \n",
    "\n",
    "**Knowledge requirements**\n",
    "\n",
    "To get the most out of this notebook you should be familiar with Python and its geoscience stack. If you only know some python that's also O.K. Most of the \"heavy lifting\" is done with our IceFlow client code so you don't necessarily need to know a lot about these libraries. If you feel like learning more about geo-science and python there are a great turorials by CU Boulder's Earth Lab here: [Data Exploration and Analysis Lessons](https://www.earthdatascience.org/tags/data-exploration-and-analysis/) or from the data carpentry project: [Introduction to Geospatial Concepts](https://datacarpentry.org/organization-geospatial/)\n",
    "\n",
    "\n",
    "The main packages that we are going to use are:\n",
    "\n",
    " * [requests](https://requests.readthedocs.io/en/master/):\n",
    " Simple HTTP library for Python, used to make requests as its name states.\n",
    " * [geopandas](https://geopandas.org/):\n",
    " library to make working with geospatial data in python easier (using pandas). \n",
    " * [geojson](https://github.com/jazzband/geojson):\n",
    " Functions for encoding and decoding GeoJSON formatted data in Python\n",
    " * [h5py](https://github.com/h5py/h5py):\n",
    " h5py is a thin, pythonic wrapper around the the [HDF5 library](https://en.wikipedia.org/wiki/Hierarchical_Data_Format). \n",
    " * [matplotlib](https://matplotlib.org/):\n",
    " Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n",
    " * [vaex](https://github.com/vaexio/vaex):\n",
    " Vaex is a high performance Python library for lazy Out-of-Core DataFrames (similar to Pandas), to visualize and explore big tabular datasets.:\n",
    " * [iPyLeaflet](https://github.com/jupyter-widgets/ipyleaflet):\n",
    " A Jupyter / Leaflet bridge enabling interactive maps in the Jupyter notebook.\n",
    " * [icepyx](https://github.com/icesat2py/icepyx):\n",
    " a software library for ICESat-2 data users\n",
    "  \n",
    "As we already mentioned, for convenience we created a python library that encapsulates the most repetitive tasks to access IceFlow and Icesat 2 data. If you feel comfortable just using code you don't need to use the map widget\n",
    "\n",
    "After completing this notebook and the companion [visualization and analysis notebook](./2_dataviz_iceflow.ipynb) you will:\n",
    "* Understand the basics about the datasets served by IceFlow;\n",
    "* Be able to access IceBridge, ICESat and ICESat-2 data using the IceFlow API;\n",
    "* Be able to read and analyze data from IceFlow.\n",
    "\n",
    "## **Why IceFlow**\n",
    "\n",
    "Short answer: **data harmonization**\n",
    "\n",
    "In 2003, NASA launched the Ice, Cloud and Land Elevation Satellite (ICESat) mission. Over the following six years, ICESat collected valuable data about ice thickness in the Polar Regions. Unfortunately, the ICESat mission ended before a follow-on mission could be launched. To fill the gap, an airborne campaign called Operation IceBridge was started. Between 2009 and 2019, Operation IceBridge flew numerous campaigns over the Greenland and Antarctic icesheets, as well as over sea ice in the Arctic and Southern Oceans. The last campaign was fill in date here. In September 2018, ICESat-2 was launched to continue NASA's collection of ice, cloud and land elevation data.\n",
    "\n",
    "The wealth of data from these three missions, as well as from pre IceBridge airborne altimetry missions, presents an opportunity to measure the evolution of ice thickness over several decades. However, combining data from these missions presented a challenge. Data from the Airborne Topographic Mapper (ATM) flown during IceBridge campaigns is store in at least 4 different file formats. ICESat and ICESat-2 data are also in different file formats. Data needs to be harmonized (put into similar formats) before comparisons can be made. A further complication is that the coordinate reference systems used to locate measurements have changed. The Earth's surface is not static and changes shape. To account for these changes, terrestrial reference frames that relate latitude and longitude to points on the Earth are updated on a regular basis. Since the launch of ICESat, the International Terrestrial Reference Frame[(ITRF)](https://www.iers.org/IERS/EN/DataProducts/ITRF/itrf.html) has been updated three times. The geolocation accuracy of instruments means that a point measured at the beginning of the record is not the same point as that measured at the end of the record. Even though the latitude and longitude is the same. These changes in geolocation need to be reconciled if meaningful comparisons of measurements are to be made.\n",
    "\n",
    "\n",
    "* A more detailed overview of these corrections can be read here: **[Applying Coordinate Transformations to Facilitate Data Comparison](https://gist.github.com/kbeamnsidc/b263eb992ce6c50a1ceafb24ac70cd0a)**\n",
    "\n",
    "\n",
    "\n",
    "### **Pre-IceBridge**\n",
    "\n",
    "The Airborne Topographic Mapper (ATM) is a conically-scanning laser altimeter that measures the surface topography of a swath of terrain directly beneath the path of the aircraft. ATM surveys can be used for change detection, with laser swaths re-surveyed after a few years, and differences between the two surveys yielding estimates of elevation change during the interim.  Comparison of surveys in 1993-4 and 1998-9 produced the first comprehensive assessment of the mass balance of the Greenland ice sheet (Krabill et al., 1999, 2000).  ATM surveys can also be used for comparison with satellite altimeter measurements for calibration/validation (e.g. Martin et al., 2005) or for determining elevation change.  The ATM has collected high quality topographic data from a wide variety of platforms, including the NASA P3, a Chilean Navy P3, a US Navy P3, the NASA DC8, the NCAR C-130, and a half-dozen Twin Otters. For a complete list of the ATM deployments you can go to [https://atm.wff.nasa.gov/deployments/](https://atm.wff.nasa.gov/deployments/)\n",
    "\n",
    "### **ICESat**\n",
    "\n",
    "ICESat (Ice, Cloud,and land Elevation Satellite) was the benchmark Earth Observing System mission for measuring ice sheet mass balance, cloud and aerosol heights, as well as land topography and vegetation characteristics. From 2003 to 2009, the ICESat mission provided multi-year elevation data needed to determine ice sheet mass balance as well as cloud property information, especially for stratospheric clouds common over polar areas. It also provided topographic and vegetation data around the globe, in addition to the polar-specific coverage over the Greenland and Antarctic ice sheets. Launched on 12 January 2003, after seven years in orbit and 18 laser-operations campaigns, the ICESat's science mission ended due to the failure of its primary instrument.\n",
    "\n",
    "\n",
    "### **IceBridge**\n",
    "\n",
    "**IceBridge** is the largest airborne survey of Earth's polar ice ever flown. It has yielded an unprecedented three-dimensional view of Arctic and Antarctic ice sheets, ice shelves and sea ice. These flights provide a yearly, multi-instrument look at the behavior of the rapidly changing features of the Greenland and Antarctic ice.\n",
    "Data collected during IceBridge helps scientists bridge the gap in polar observations between NASA's Ice, Cloud and Land Elevation Satellite (ICESat) -- launched in 2003 and de-orbited in 2010 -- and ICESat-2, launched in 2018. ICESat stopped collecting science data in 2009, making IceBridge critical for extending the ice altimetry time series in the Arctic and Antarctica, although the data are not continuous.\n",
    "\n",
    "IceBridge flights were generally conducted in **March-May over Greenland and in October-November over Antarctica**.\n",
    "\n",
    "### **ICESat 2**\n",
    "\n",
    "The ICESat-2 mission is designed to provide elevation data needed to determine ice sheet mass balance as well as vegetation canopy information. It will provide topographic measurements of cities, lakes and reservoirs, oceans and land surfaces around the globe. The sole instrument on ICESat-2 is the Advanced Topographic Laser Altimeter System (ATLAS), a space-based Lidar. It was designed and built at Goddard Space Flight Center, with the laser generation and detection systems provided by Fibertek. ATLAS measures the travel time of laser photons from the satellite to Earth and back; computer programs use the travel time from multiple pulses to determine elevation.\n",
    "\n",
    "You can go to NSIDC's landing page for a complete list of IceSat 2 datasets and their documentation: [ICESat-2 Data Sets at NSIDC](https://nsidc.org/data/icesat-2/data-sets)\n",
    "\n",
    "<p align=\"center\">\n",
    "<img style=\"align: center;\" width=\"80%\" src='./img/iceflow-coverage.jpg'/>\n",
    "    <br>\n",
    "    <b>Fig 2. IceFlow mission coverages</b>\n",
    "</p>\n",
    "\n",
    "\n",
    "## Data sets and their coverage\n",
    "\n",
    "\n",
    "The IceFlow project provides web services for ordering spatially and temporally subseted Lidar point cloud data from the [BLATM L1B](https://nsidc.org/data/BLATM1B), [ILATM L1B v1](https://nsidc.org/data/ilatm1b/versions/1), [ILATM L1B V2](https://nsidc.org/data/ILATM1B), [ILVIS2](https://nsidc.org/data/ILVIS2) and [IceSat GLAH06](https://nsidc.org/data/GLAH06/) data products.\n",
    "\n",
    "The following table describes the temporal and spatial coverage of each of these dataset as well as the sensor and platform used to acquire the data.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "|              | Spatial Coverage                                                      | Temporal Coverage                              | Platform                                              | Sensor                   |\n",
    "|--------------|-----------------------------------------------------------------------|------------------------------------------------|-------------------------------------------------------|--------------------------|\n",
    "| [BLATM L1B](https://nsidc.org/data/BLATM1B)    | South: N:-53, S: -90, E:180, W:-180 North: N:90, S: 60, E:180, W:-180 | 23 June 1993 - 30 October 2008                 | DC-8, DHC-6, P-3A ORION, P-3B                         | ATM                      |\n",
    "| [ILATM L1B v1](https://nsidc.org/data/ilatm1b/versions/1) | South: N:-53, S: -90, E:180, W:-180 North: N:90, S: 60, E:180, W:-180 | 31 March 2009 - 8 November 2012 (updated 2013) | AIRCRAFT, DC-8, P-3B                                  | ATM                      |\n",
    "| [ILATM L1B V2](https://nsidc.org/data/ILATM1B)| South: N:-53, S: -90, E:180, W:-180 North: N:90, S: 60, E:180, W:-180 | 20 March 2013 - 16 May 2019 (updated 2020)     | C-130, DC-8, HU-25A, HU-25C, P-3B, WP-3D ORION        | ATM                      |\n",
    "| [ILVIS2](https://nsidc.org/data/ILVIS2)       | North: N:90, S: 60, E:180, W:-180                                     | 25 August 2017 - 20 September 2017             | AIRCRAFT, B-200, C-130, DC-8, G-V, HU-25C, P-3B, RQ-4 | ALTIMETERS, LASERS, LVIS |\n",
    "| [GLAH06](https://nsidc.org/data/GLAH06/)       | Global: N:86, S: -86, E:180, W:-180                                     |     20 February 2003 - 11 October 2009        | IceSat | ALTIMETERS, CD, GLAS, GPS, GPS Receiver, LA, PC\n",
    "\n",
    "\n",
    "--- \n",
    "\n",
    "> **Note**: If you have any qustions about the data please contact NSIDC user services at nsidc@nsidc.org\n",
    "\n",
    "In this tutorial we are going to use iPyLeaflet and other Jupyter widgets to select our constraints and place a data order using the IceFlow API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NASA's EarthData Credentials\n",
    "\n",
    "The first step to start working with IceFlow data is to login into [NASA's Earthdata Search](https://earthdata.nasa.gov/).\n",
    "\n",
    "> **Note**: you don't need to be a NASA employee to register with NASA EarthData!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We import our IceFlow client library\n",
    "from iceflow.client import IceflowClient\n",
    "# We instantiate our client\n",
    "iceflow = IceflowClient()\n",
    "# We need to use our NASA Earth Data Credentials and verify that they work.\n",
    "# Please click on set credentials and then see if we are authenticated by executing the next cell.\n",
    "iceflow.display(['credentials'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to verify that our credentials are valid.\n",
    "session = iceflow.create_earthdata_authenticated_session()\n",
    "if session is None:\n",
    "    print('we are not logged into NASA EarthData')\n",
    "else:\n",
    "    print('we are logged into NASA EarthData!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with the user interface. We'll explain what this does next.\n",
    "# vertical = Sidecar widget, horizontal = render the widget in this notebook.\n",
    "iceflow.display(['map'], 'vertical', extra_layers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This user interface uses [ipylaflet](https://blog.jupyter.org/interactive-gis-in-jupyter-with-ipyleaflet-52f9657fa7a) which allows us to draw\n",
    "polygons or bounding boxes to delimit our area of interest. We can also edit and delete these geometries using the the widget controls in the map.\n",
    "\n",
    "The **\"Get Granule Count\"** button will query [NASA's CMR](https://earthdata.nasa.gov/eosdis/science-system-description/eosdis-components/cmr) to get a granule count for the current parameters, we need to have a geometry and one or more datasets selected.\n",
    "\n",
    "**Notes**: \n",
    "> * If you use the bounding box geometry in a polar projection you'll notice a distortion due the nature of polar coordinates, if you prefer you can use the global mercator map to draw a bounding box without apparent distortion. Polygons are probably a better idea or you can even input your own coordinates as we'll see later.\n",
    "> * The calculated download size of these granules is an upper bound since IceFlow allows us to subset the data. \n",
    "\n",
    "## IceFLow notebook interface components\n",
    "\n",
    "* **Hemisphere**: Which map projection we are going to use, we can pick global, north or south\n",
    "* **Datasets**: A selection of the datasets served by IceFlow, we can pick one or more (CTRL+ Space or CTRL+Click) In the widget ATM includes the 3 different ATM products (BLATM L1B, ILATM L1B v1, ILATM L1B V2)\n",
    "* **IceSat2**: If we want to also place a data order for IceSat 2 data for the current parameters we need to select the short name code i.e. ATL06\n",
    "* **ITRF**: The International Terrestrial Reference Frame see: [ITRF](https://gist.github.com/kbeamnsidc/b263eb992ce6c50a1ceafb24ac70cd0a)\n",
    "* **Epoch**: The epoch in which the coordinate reference systems are based, valid when using ITRF.\n",
    "* **Date Range**: A slider control to filter the selection area between a start and end date\n",
    "* **Map**: The main widget, you can draw polygon or bounding boxes and edit them. You can also turn on and off the layers that show IceBridge flights and the rest.\n",
    "\n",
    "To display the user interface we use the method `display` and we need 3 parameters:\n",
    "\n",
    "* **what**: what we want to display, the valid values are **credentials**, **controls** and **map**\n",
    "* **where**: where we want the widgets to be, horizontal will render them in the next cell or vertical will use a separate column.\n",
    "* **extra_layers**: if we want to add more than the IceBridge layers to the map, True or False\n",
    "\n",
    "> **Note:** The IceFlow client can work directly with the data ordering system. If you are logged into NASA you can just build your own parameters and send them directly without using the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NASA Common Metadata Repository (CMR)\n",
    "\n",
    "What is CMR?\n",
    "\n",
    "NASA's Common Metadata Repository (CMR) is a high-performance, high-quality, continuously evolving metadata system that catalogs all data and service metadata records for NASA's Earth Observing System Data and Information System (EOSDIS) system and will be the authoritative management system for all EOSDIS metadata. These metadata records are registered, modified, discovered, and accessed through programmatic interfaces leveraging standard protocols and APIs. \n",
    "\n",
    "**In short: NASA's CMR is a \"database\" for Earth-related datasets, in this case all the data served by IceFlow is also indexed by CMR**\n",
    "\n",
    "> **Note**: One **important** thing to notice here is that CMR has the location of the original data granules and thus they are in multiple data formats and projections. Again, here is where tools like IceFlow come handy. Also, the data size calculation for CMR granules is an upper bound since CMR does not subsets the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the widget statate to build spatio-temporal parameters\n",
    "params = iceflow.build_params()\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can query CMR to get an idea of coverages for the area we just selected.\n",
    "# The granules and total size is an upper bound since CMR has full granules\n",
    "# and IceFlow will subset them to only cover the area selected.\n",
    "# datasets = ['BLATM1B', 'GLAH06', 'ILVIS2', 'ATL06']\n",
    "datasets = ['BLATM1B', 'GLAH06', 'ILVIS2']\n",
    "granules = iceflow.query_cmr(datasets, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## We can print the first record for a given dataset\n",
    "dataset = 'BLATM1B'\n",
    "if len(granules[dataset])>0:\n",
    "    print(granules[dataset][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IceFlow API\n",
    "\n",
    "The native IceFlow API offers us an Open API specification that we can use to explore how the service works. \n",
    "\n",
    "IceFlow has 3 dataset end-points and endpoints to check the status of previous data orders. Each data set endpoint allows users to specify their spatial and temporal extent. \n",
    "\n",
    "**Spatial Extent Parameters**\n",
    "\n",
    "* **Polygon**: A conterclockwise closed array of lat-lons, the last coordinate has to be the same as the first pair.\n",
    "* **bounding box**: a WGS84 box with min_lon,min_lat,max_lon,max_lat values\n",
    "\n",
    "**Temporal Extent Parameters**\n",
    "* **date_range**: The date/time range over which to return data, accepts UTF datetime or simple YYYY-mm-dd formatted values.\n",
    "\n",
    "**ITRF** (optional)\n",
    "* The ITRF reference to which the data will be transformed via the published ITRF transformation parameters. Optional, but must be used when specifying epoch.\n",
    "* Available values : **ITRF2000, ITRF2008, ITRF2014**\n",
    "\n",
    "**Epoch** (optional)\n",
    "* The epoch (in decimal years) to which the data will be transformed via the ITRF Plate Motion Model corresponding to ITRF. Optional, and can be used when also specifying itrf, but can only be used if itrf is ITRF2008 or ITRF2014 since there is no ITRF2000 Plate Motion Model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('http://valkyrie-vm.apps.nsidc.org/1.0/ui/#/', width=800, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placing a data order for one of the data sets served by IceFlow\n",
    "\n",
    "Now that we have our constraints we just need to post our order and wait for IceFlow to fulfill it. \n",
    "\n",
    "We can put an order directly, in this case we are going to work on a geometry that overlaps with [Jakobshavn](https://en.wikipedia.org/wiki/Jakobshavn_Glacier) glacier in Greenland or Thwaites Glacier in Antarctica (depending on which one you select). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that we are explicitly using the name of one of the ATM datasets\n",
    "# dataset = 'ILATM1B'\n",
    "dataset = 'GLAH06'\n",
    "\n",
    "# Thwaites glacier\n",
    "my_params ={\n",
    "    'dataset': 'GLAH06',\n",
    "    'start': '2001-01-01',\n",
    "    'end': '2011-1-01',\n",
    "    'bbox': '-107.4515,-75.3695,-105.3794,-74.4563'\n",
    "}\n",
    "\n",
    "# Jakobshavn 2016\n",
    "# my_params ={\n",
    "#     'dataset': dataset,\n",
    "#     'start': '2016-01-01',\n",
    "#     'end': '2016-12-31',\n",
    "#     'bbox': '-107.4515,-75.3695,-105.3794,-74.4563'\n",
    "# }\n",
    "\n",
    "# returns a json dictionary, the request parameters and the order's response.\n",
    "order = iceflow.post_iceflow_order(my_params)\n",
    "order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "Let's get some coffee, some IceFlow orders are in the Gigabytes real and may take a little while to be processed. \n",
    "Once that your status URL says is completed we can grab the HDF5 data file using the URL on the same response!\n",
    "\n",
    "The first thing we need to know is the order status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order['response'].json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_status = iceflow.order_status(order)\n",
    "if 'url' in order_status:\n",
    "    data_url = order_status['url']\n",
    "order_status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data order is complete\n",
    "Now that we know the order is complete we can download the file in our status response `url` to our local working directory using requests or curl or wget or just clicking the link.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can name our file or leave it with the original file name by not using the file_name parameter.\n",
    "file_name = 'twaties-test-GLAH06-2000-2010'\n",
    "iceflow_file = iceflow.download_order(data_url, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Placing multiple data orders to IceFlow\n",
    "\n",
    "If we need more than one data set from IceFlow at the same time, we can use the client to place our data orders, IceFlow will return a single HDF5 file for each of the data sets that we have selected using the map widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# orders = v.post_orders()\n",
    "# orders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Related Datasets\n",
    "\n",
    "* **TODO**: [IcePyx](https://github.com/icesat2py/icepyx/blob/development/examples/ICESat-2_DAAC_DataAccess2_Subsetting.ipynb) + IceFlow Downloader example: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the Data\n",
    "\n",
    "Remote sensing data can be overwhelmingly big. Reading a big file is not trivial and when we have an array of them this task can become an intractable barrier.\n",
    "The main constraint if you don't have a super computer is memory. The average granule size is in the 10s of MB for IceSat 2 and could be Gigabytes in IceFlow depending on the selected area. This is when libraries like Dask, Vaex and others come into play. \n",
    "\n",
    "These libraries read our files using a battery of optimizations like lazy loading, memory mapping and parallelism. Let's now explore 4 different ways of reading these HDF5 files using libraries included in this notebook:\n",
    "\n",
    "* h5py + Pandas\n",
    "* Dask arrays\n",
    "* Vaex\n",
    "* xarray\n",
    "\n",
    "## What's in the IceFlow HDF5 file?\n",
    "\n",
    "**TODO**: write about the important fields on the datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## h5py + pandas\n",
    "\n",
    "With h5py we get almost native access to the hdf5 files and we can use pandas or geopandas to compute operations on them.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing all of our dependencies\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import glob\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import vaex\n",
    "import dask.dataframe as dd\n",
    "import dask.array as da\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# file_name = 'data/atm1b_data_2020-07-11T20-39.hdf5'\n",
    "file_name = 'data/twaties-test-GLAH06-2000-2010.h5'\n",
    "\n",
    "f = h5py.File(file_name, 'r')\n",
    "# We are going to print our variables\n",
    "print(list(f.keys()))\n",
    "\n",
    "glas_data = {\n",
    "    'latitude': f['d_lat'],\n",
    "    'longitude': f['d_lon'],\n",
    "    'elevation': f['d_elev'],\n",
    "    'time': pd.to_datetime(f['utc_datetime'])\n",
    "}\n",
    "\n",
    "# Dictionary for ATM data\n",
    "# df_data = {\n",
    "#     'latitude': f['latitude'],\n",
    "#     'longitude': f['longitude'],\n",
    "#     'elevation': f['elevation'],\n",
    "#     'time': pd.to_datetime(f['utc_datetime'])\n",
    "# }\n",
    "\n",
    "# Dictionary for ATM data\n",
    "df = pd.DataFrame(data=glas_data)\n",
    "display(df)\n",
    "# compute the mean elevation subsetting by coordinates\n",
    "df[ df.latitude < -74.939994 ].elevation.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Xarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import xarray as xr\n",
    "fname = 'data/atm1b_data_2020-07-11T20-39.hdf5'\n",
    "import xarray as xr\n",
    "ds = xr.open_dataset(fname) \n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import vaex\n",
    "df = vaex.open('data/atm1b_data_2020-07-11T20-39.hdf5')\n",
    "# We're parsing the utc_datetime from IceFlow into a data type that Vaex understands.\n",
    "df['date'] = df.utc_datetime.values.astype('datetime64[ns]')\n",
    "my_df = df['longitude', 'latitude', 'elevation', 'date']\n",
    "my_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "1. [Airborne Topographic Mapper Calibration Procedures and Accuracy Assessment](https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20120008479.pdf)\n",
    "\n",
    "2. [Open Source Tools for Point Cloud Processing, Storage, Subsetting, and Visualization](https://sea.ucar.edu/sites/default/files/kbeam_seaconf18.pdf)\n",
    "\n",
    "### Related Tools\n",
    "\n",
    "* [OpenAltimetry](https://openaltimetry.org/): Advanced discovery, processing, and visualization services for ICESat and ICESat-2 altimeter data\n",
    "* [ITS_LIVE](https://its-live.jpl.nasa.gov/):A NASA MEaSUREs project to provide automated, low latency, global glacier flow and elevation change datasets. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
