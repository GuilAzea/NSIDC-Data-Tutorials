{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snow Depth and Snow Cover Data Exploration \n",
    "\n",
    "This tutorial demonstrates how to access and compare coincident snow data across in-situ, airborne, and satellite platforms from NASA's SnowEx, ASO, and xx data sets, respectively. All data are available from the NASA National Snow and Ice Data Center Distributed Active Archive Center, or NSIDC DAAC. \n",
    "\n",
    "## Here are the steps you will learn in this snow data notebook:\n",
    "\n",
    "\n",
    "* Explore the coverage, resolution, and structure of select NSIDC DAAC snow data products, as well as available resources to search and access data.\n",
    "* Learn how to find and download spatiotemporally coincident data across in-situ, airborne, and satellite observations.\n",
    "* Learn how to read data into Python from CSV, GeoTIFF, and HDF-EOS formats.\n",
    "*Learn how to extract and compare raster values at point locations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "## Explore snow products and resources\n",
    "\n",
    "\n",
    "### NSIDC introduction\n",
    "\n",
    "[The National Snow and Ice Data Center](https://nsidc.org) provides over 1100 data sets covering the Earth's cryosphere and more, all of which are available to the public free of charge. Beyond providing these data, NSIDC creates tools for data access, supports data users, performs scientific research, and educates the public about the cryosphere. \n",
    "\n",
    "#### Data Resources\n",
    "\n",
    "* [NSIDC Data Search](https://nsidc.org/data/search/#keywords=snow)\n",
    "    * Search NSIDC snow data\n",
    "* [NSIDC Data Update Announcements](https://nsidc.org/the-drift/data-update/) \n",
    "    * News and tips for data users\n",
    "* [NASA Earthdata Search](http://search.earthdata.nasa.gov/)\n",
    "    * Search and access data across the NASA Earthdata\n",
    "* [NASA Worldview](https://worldview.earthdata.nasa.gov/)\n",
    "    * Interactive interface for browsing full-resolution, global, daily satellite images\n",
    "    \n",
    "    \n",
    "### Snow Today\n",
    "\n",
    "[Snow Today](https://nsidc.org/snow-today), a collaboration with the University of Colorado's Institute of Alpine and Arctic Research (INSTAAR), provides near-real-time snow analysis for the western United States and regular reports on conditions during the winter season. Snow Today is funded by NASA Hydrological Sciences Program and utilizes data from the\n",
    "Moderate Resolution Imaging Spectroradiometer (MODIS)instrument and snow station data from the Snow Telemetry (SNOTEL) network by the Natural Resources Conservation Service (NRCS), United States Department of Agriculture (USDA) and the California Department of Water Resources: www.wcc.nrcs.usda.gov/snow.\n",
    "\n",
    "* Say something about below average snow pack across Western US even with record snowfall in Colorado this year. Could lead into interest in focusing on Grand Mesa...\n",
    "\n",
    "\n",
    "### Snow-related missions and data sets used in the following steps:\n",
    "\n",
    "* [SnowEx](https://nsidc.org/data/snowex)\n",
    "    * SnowEx17 Ground Penetrating Radar Version 2: https://doi.org/10.5067/G21LGCNLFSC5\n",
    "\n",
    "* [ASO](https://nsidc.org/data/aso)\n",
    "    * L4 Lidar Snow Depth 50m UTM Grid Version 1: https://doi.org/10.5067/STOT5I0U1WVI\n",
    "    * ASO L4 Lidar Snow Depth 3m UTM Grid Version 1: https://doi.org/10.5067/KIE9QNVG7HP0\n",
    "* [MODIS](https://nsidc.org/data/modis)\n",
    "    * MODIS/Terra Snow Cover Daily L3 Global 500m SIN Grid, Version 6: https://doi.org/10.5067/MODIS/MOD10A1.006\n",
    "\n",
    "\n",
    "#### Other popular snow products:\n",
    "\n",
    "* [VIIRS](https://nsidc.org/data/viirs)\n",
    "    * (VIIRS/NPP CGF Snow Cover Daily L3 Global 375m SIN Grid) V1: https://doi.org/10.5067/VIIRS/VNP10A1F.001\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "\n",
    "## Data Discovery\n",
    "\n",
    "Using this project in EDSC: \n",
    "https://search.earthdata.nasa.gov/projects?projectId=4546673848\n",
    "\n",
    "\n",
    "SNOTEL data from https://www.wcc.nrcs.usda.gov/snow/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Import Packages\n",
    "\n",
    "Get started by importing packages needed to run the following code blocks, including the `tutorial_helper_function` module provided within this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import fiona\n",
    "from shapely.geometry import Polygon, mapping\n",
    "from shapely.geometry.polygon import orient\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio\n",
    "from rasterio.plot import show\n",
    "import numpy as np\n",
    "import pyresample as prs\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import getpass\n",
    "from rasterio.mask import mask\n",
    "\n",
    "\n",
    "# This is our functions module. We created several helper functions to discover, access, and harmonize the data below.\n",
    "import tutorial_helper_functions as fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify area and time of interest\n",
    "\n",
    "Since our focus is on the Grand Mesa study site of the NASA SnowEx campaign, we'll use that area to search for coincident data across other data products. From the [SnowEx17 Ground Penetrating Radar Version 2](https://doi.org/10.5067/G21LGCNLFSC5) landing page, you can find the rectangular spatial coverage under the Overview tab, or you can draw a polygon over your area of interest in the map under the Download Data tab and export the shape as a geojson file using the Export Polygon icon shown below. An example polygon geojson file is provided in the /Data folder of this repository.   \n",
    "\n",
    "<img align=\"left\" src=\"Data-download-polygon-export.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create polygon coordinate string\n",
    "\n",
    "Read in the geojson file as a GeoDataFrame object and simplify and reorder using the shapely package. This will be converted back to a dictionary to be applied as our polygon search parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polygon coordinates to be used in search: -108.2352445938561,38.98556907427165,-107.85284607930835,38.978765032966244,-107.85494925720668,39.10596902171742,-108.22772795408136,39.11294532581687,-108.2352445938561,38.98556907427165\n"
     ]
    },
    {
     "data": {
      "image/svg+xml": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"100.0\" height=\"100.0\" viewBox=\"-108.250540534438 38.96346909238434 0.41299039571156015 0.16477217401443767\" preserveAspectRatio=\"xMinYMin meet\"><g transform=\"matrix(1,0,0,-1,0,78.0917103587831)\"><path fill-rule=\"evenodd\" fill=\"#66cc99\" stroke=\"#555555\" stroke-width=\"0.008259807914231204\" opacity=\"0.6\" d=\"M -108.2352445938561,38.98556907427165 L -107.85284607930835,38.978765032966244 L -107.85494925720668,39.10596902171742 L -108.22772795408136,39.11294532581687 L -108.2352445938561,38.98556907427165 z\" /></g></svg>"
      ],
      "text/plain": [
       "<shapely.geometry.polygon.Polygon at 0x11f573c88>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "polygon_filepath = str(os.getcwd() + '/Data/nsidc-polygon.json') # Note: A shapefile or other vector-based spatial data format could be substituted here.\n",
    "\n",
    "gdf = gpd.read_file(polygon_filepath) #Return a GeoDataFrame object\n",
    "\n",
    "poly = gdf.loc[0] # Label based indexing of GeoDataFrame object to get it into a shapeply geometry object.\n",
    "\n",
    "# Simplify polygon for complex shapes in order to pass a reasonable request length to CMR. The larger the tolerance value, the more simplified the polygon.\n",
    "poly = poly.simplify(0.05, preserve_topology=False)\n",
    "\n",
    "poly = orient(poly, sign=1.0) # Orient counter-clockwise: CMR polygon points need to be provided in counter-clockwise order. The last point should match the first point to close the polygon.\n",
    "\n",
    "#Format dictionary to polygon coordinate pairs for CMR polygon filtering\n",
    "polygon = ','.join([str(c) for xy in zip(*poly.exterior.coords.xy) for c in xy])\n",
    "print('Polygon coordinates to be used in search:', polygon)\n",
    "poly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set time range\n",
    "\n",
    "We are interested in accessing files within each data set over the same time range, so we'll start by searching all of 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal = '2017-01-01T00:00:00Z,2017-12-31T23:59:59Z' # Set temporal range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create data dictionary \n",
    "\n",
    "Create a nested dictionary with each data set short name and version, as well as shared temporal range and polygonal area of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = { 'snowex': {'short_name': 'SNEX17_GPR','version': '2','polygon': polygon,'temporal':temporal},\n",
    "                'aso': {'short_name': 'ASO_3M_SD','version': '1','polygon': polygon,'temporal':temporal},\n",
    "             'viirs': {'short_name': 'MOD10A1','version': '6','polygon': polygon,'temporal':temporal}\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine how many files exist over this time and area of interest, as well as the average size and total volume of those files\n",
    "\n",
    "We will use the `granule_info` function to query metadata about each data set and associated files using the [Common Metadata Repository (CMR)](https://cmr.earthdata.nasa.gov/search/site/docs/search/api.html), which is a high-performance, high-quality, continuously evolving metadata system that catalogs Earth Science data and associated service metadata records. Note that not all NSIDC data can be searched at the file level using CMR, particularly those outside of the NASA DAAC program. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in data_dict.items(): fn.granule_info(data_dict[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find coincident data\n",
    "\n",
    "The function above tells us the size of data available for each data set over our time and area of interest, but we want to go a step further and determine what time ranges are coincident based on our bounding box. This `time_overlap` helper function returns a dataframe with file names, dataset_id, start date, and end date for all files that overlap in temporal range across all data sets of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fn.time_overlap(data_dict)\n",
    "print(len(df), ' total files returned')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Access\n",
    "\n",
    "We have greatly reduced the number of files needed to compare data across these data sets. For the next steps, we'll focus on a single day: 8 Feb 2017... We'll now collect the data file URLs and download each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create new dictionary with fields needed for CMR url search\n",
    "\n",
    "url_df = df.drop(columns=['start_date', 'end_date','version','dataset_id'])\n",
    "url_dict = url_df.to_dict('records')\n",
    "\n",
    "# CMR search variables\n",
    "granule_search_url = 'https://cmr.earthdata.nasa.gov/search/granules'\n",
    "headers= {'Accept': 'application/json'}\n",
    "\n",
    "# Create URL list from each df row\n",
    "urls = []\n",
    "for i in range(len(url_dict)):\n",
    "    response = requests.get(granule_search_url, params=url_dict[i], headers=headers)\n",
    "    results = json.loads(response.content)\n",
    "    urls.append(fn.cmr_filter_urls(results))\n",
    "# flatten url list\n",
    "urls = list(np.concatenate(urls))\n",
    "urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will download all URLs in the list we gathered in the previous block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = str(os.getcwd() + '/Data')\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)\n",
    "os.chdir(path)\n",
    "#fn.cmr_download(urls)\n",
    "fn.cmr_download(urls[0:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in SnowEx data and buffer points around Snotel location\n",
    "\n",
    "SnowEx provided in CSV. We'll use pandas dataframe to pull in data... We'll just pick one day of data and grab the coincident ASO and VIIRS data for this demonstration\n",
    "\n",
    "\n",
    "We will be focusing on two packages to read in our data:\n",
    "\n",
    "* [Pandas DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/overview.html)\n",
    "    * Pandas is built on top of NumPy and provides easy to work with data structures. The pandas dataframe is a 2-D tabular data structure with labeled axes. \n",
    "* [Xarray](http://xarray.pydata.org/en/stable/index.html)\n",
    "    * Designed for multidimensional data. Xarray introduces labels in the form of dimensions, coordinates and attributes on top of raw NumPy-like arrays. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snowex_path = './SnowEx17_GPR_Version2_Week1.csv' # Define local filepath\n",
    "df = pd.read_csv(snowex_path, sep='\\t') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to documentation:\n",
    "\n",
    "| Variable     | Description                        |\n",
    "|--------------|------------------------------------|\n",
    "| GPR          | Short for Ground Penetrating Radar |\n",
    "| [LineNumber] | Value associated with the raw file |\n",
    "| [date]       | Date in mmddyy format              |\n",
    "| TRACE     | Trace number corresponding to the raw data files; each trace is the individual recording of a received signal from the GPR pulse |                 \n",
    "| LONG      | Longitude                                                                                                                        |\n",
    "| LAT       | Latitude                                                                                                                         |\n",
    "| ELEV      | Elevation, in meters (m)                                                                                                         |\n",
    "| TWTT      | Two-way travel time, in nanoseconds (ns), of the interpreted ground surface reflection                                           |\n",
    "| THICKNESS | Snow depth, in meters (m)                                                                                                        |\n",
    "| SWE       | Snow water equivalent (SWE), in millimeters (cm)                                                                                 |\n",
    "| X         | Easting, calculated from the recorded longitude                                                                                  |\n",
    "| Y         | Northing, calculated form the recorded latitude                                                                                  |\n",
    "| UTM_ZONE  | UTM grid zone for the calculated easting and northing                                                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to time values and extract a single day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pull out collection date:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = df.collection.str.rsplit('_').str[-1].astype(str)\n",
    "df.date = pd.to_datetime(df.date, format=\"%m%d%y\")\n",
    "df = df.sort_values(['date'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create new dataframe for February 8:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = df[df['date'] == '2017-02-08'] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to Geopandas dataframe to provide point geometry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf = gpd.GeoDataFrame(\n",
    "#    df_subset, geometry=gpd.points_from_xy(df_subset.long, df_subset.lat))\n",
    "\n",
    "# create separate gdf in UTM CRS for buffer distance calculation\n",
    "#gdf = gpd.GeoDataFrame(df_subset, geometry=gpd.points_from_xy(df_subset.long, df_subset.lat), crs='EPSG:4326')\n",
    "gdf_utm= gpd.GeoDataFrame(df_subset, geometry=gpd.points_from_xy(df_subset.x, df_subset.y), crs='EPSG:32612')\n",
    "gdf_utm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buffer data around SNOTEL site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further subset to get within 500 m radius of SNOTEL Mesa Lakes site at 39.05 -108.067 according to https://wcc.sc.egov.usda.gov/nwcc/site?sitenum=622&state=co\n",
    "\n",
    "First we'll create a new GeoDataFrame with our SNOTEL site location, set to our SnowEx UTM coordinate reference system and create a 500 meter buffer around this point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another geodataframe (gdfsel) with the center point for the selection\n",
    "df_snotel = pd.DataFrame(\n",
    "    {'SNOTEL Site': ['Mesa Lakes'],\n",
    "     'Latitude': [39.05],\n",
    "     'Longitude': [-108.067]})\n",
    "gdf_snotel = gpd.GeoDataFrame(df_snotel, geometry=gpd.points_from_xy(df_snotel.Longitude, df_snotel.Latitude), crs='EPSG:4326')\n",
    "\n",
    "gdf_snotel.to_crs('EPSG:32612', inplace=True) # set CRS to UTM 12 N\n",
    "\n",
    "#create 500 m buffer\n",
    "buffer = gdf_snotel.buffer(500)\n",
    "\n",
    "gdf_snotel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subset SnowEx points to buffer and convert to WGS84 CRS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the within method of geopandas to find the points within. E.g. gdf_within =\n",
    "# gdfdata.loc[gdfdata.geometry.within(gdfselbuff.unary_union)]\n",
    "\n",
    "gdf_buffer = gdf_utm.loc[gdf_utm.geometry.within(buffer.unary_union)]\n",
    "gdf_buffer = gdf_buffer.to_crs('EPSG:4326')\n",
    "# print(len(gdf_utm))\n",
    "# print(len(gdf_buffer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in Airborne Snow Observatory data\n",
    "\n",
    "ASO provided as GeoTIFF so we'll use Rasterio. The read() method returns a Numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aso_path = './ASO_3M_SD_USCOGM_20170208.tif' # Define local filepath\n",
    "#aso_path = './ASO_50M_SD/ASO_50M_SD_USCOGM_20170220.tif' # Define local filepath\n",
    "#aso_qual_path = './ASO_3M_SD/ASO_3M_QF_USCOGM_20170221.tif' # Define local filepath for associated quality file\n",
    "\n",
    "aso = rasterio.open(aso_path)\n",
    "# aso_array = aso.read(1, masked=True)\n",
    "#qual = ASO_3M_QF_USCOGM_20170208.read(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clip data to SNOTEL buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#buffer = buffer.to_crs(crs=aso.crs.data)\n",
    "buffer = buffer.to_crs(crs=aso.crs) # convert buffer to CRS of ASO rasterio object\n",
    "\n",
    "#new_raster = rasterio.mask.mask(aso, buffer, all_touched=False, invert=False, nodata=None, filled=True, crop=False, pad=False, pad_width=0.5, indexes=None)\n",
    "out_img, out_transform = mask(aso, buffer, crop=True)\n",
    "\n",
    "out_meta = aso.meta.copy()\n",
    "\n",
    "epsg_code = int(aso.crs.data['init'][5:])\n",
    "\n",
    "out_meta.update({\"driver\": \"GTiff\", \"height\": out_img.shape[1], \"width\": out_img.shape[2], \"transform\": out_transform, \"crs\": '+proj=utm +zone=13 +datum=WGS84 +units=m +no_defs'})\n",
    "\n",
    "out_tif = 'clipped_ASO_3M_SD_USCOGM_20170208.tif'\n",
    "\n",
    "with rasterio.open(out_tif, 'w', **out_meta) as dest:\n",
    "    dest.write(out_img)\n",
    "    \n",
    "clipped_aso = rasterio.open(out_tif)\n",
    "\n",
    "#show(clipped, cmap='viridis')\n",
    "\n",
    "aso_array = clipped_aso.read(1, masked=True)\n",
    "\n",
    "#clipped_array[clipped_array == -9999.0] = np.nan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add ASO data to GeoPandas dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projection and extent metadata:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(clipped_aso.profile)\n",
    "print('')\n",
    "print(clipped_aso.bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# from pyresample import create_area_def\n",
    "# area_id = 'UTM_13N'\n",
    "# proj_string = '+proj=utm +zone=13 +datum=WGS84 +units=m +no_defs'\n",
    "# center = (244510.5, 4323769.5)\n",
    "# # >>> radius = (5326849.0625, 5326849.0625)\n",
    "# # >>> resolution = (25067.525, 25067.525)\n",
    "# create_area_def(area_id, proj_string, center=center)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define lat lon swath geometry used for interpolation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # \n",
    "# snowex_geometry = prs.geometry.SwathDefinition(lons=gdf['long'], lats=gdf['lat'])\n",
    "# print('snowex geometry: ', snowex_geometry)\n",
    "\n",
    "snowex_geometry = prs.geometry.SwathDefinition(lons=gdf_buffer['long'], lats=gdf_buffer['lat'])\n",
    "#print('snowex geometry: ', snowex_geometry)\n",
    "\n",
    "\n",
    "# Create area definition for ASO\n",
    "area_id = 'UTM_13N' # area_id: ID of area\n",
    "description = 'WGS 84 / UTM zone 13N' # description: Description\n",
    "proj_id = 'UTM_13N' # proj_id: ID of projection (being deprecated)\n",
    "#projection = 'PROJCS[\"WGS 84 / UTM zone 12N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",-111],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32612\"]]'\n",
    "projection = 'EPSG:32613' # projection: Proj4 parameters as a dict or string\n",
    "# projection = {'proj': 'utm', 'zone': '13', 'datum': 'WGS84', 'units': 'm'}\n",
    "#proj_string = '+proj=utm +zone=13 +datum=WGS84 +units=m +no_defs'\n",
    "#+proj=utm +zone=13 +datum=WGS84 +units=m +no_defs \n",
    "width = clipped_aso.width # width: Number of grid columns\n",
    "height = clipped_aso.height # height: Number of grid rows\n",
    "# from aso.bounds; area_extent: (lower_left_x, lower_left_y, upper_right_x, upper_right_y)\n",
    "area_extent = (234081.0, 4326303.0, 235086.0, 4327305.0)\n",
    "aso_geometry = prs.geometry.AreaDefinition(area_id, description, proj_id, projection, width, height, area_extent)\n",
    "print ('aso geometry: ', aso_geometry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolate ASO values onto SnowEx points\n",
    "\n",
    "To easily interpolate ASO snow depth to SnowEx points, we can use the `pyresample` library. The `radius_of_influence` parameter determines maximum radius to look for nearest neighbor interpolation.\n",
    "\n",
    "Masked arrays can be used as data input. In order to have undefined pixels masked out instead of assigned a fill value set fill_value=None when calling the resample_* function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add ASO values to geodataframe\n",
    "gdf_buffer['aso_snow_depth'] = prs.kd_tree.resample_nearest(aso_geometry, aso_array, snowex_geometry, radius_of_influence=3)\n",
    "\n",
    "#gdf_buffer[gdf_buffer['aso_snow_depth'] == -9999.0] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buffer.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add difference column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buffer['diff'] = abs(gdf_buffer['Thickness'] - gdf_buffer['aso_snow_depth'])\n",
    "gdf_buffer.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buffer['diff'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GeoTIFF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show(clipped_aso, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buffer_aso_crs = gdf_buffer.to_crs('EPSG:32613')\n",
    "#gdf_buffer = gdf_buffer.to_crs('EPSG:32613')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_buffer_aso_crs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.scatter(atl07.longitude, atl07.latitude,\n",
    "#           c=atl07.height_segment_height, vmax=1.5,\n",
    "#           cmap='Reds', alpha=0.6, s=1)    \n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "show(clipped_aso, ax=ax)\n",
    "divider = make_axes_locatable(ax)\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.1)\n",
    "gdf_buffer_aso_crs.plot(column='Thickness', ax=ax, cmap='OrRd', legend=True, cax=cax, legend_kwds=\n",
    "                        {'label': \"Snow Depth (m)\",});\n",
    "#edgecolor='r', facecolor='none',"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move to GIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write to shapefile for GIS applications:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gdf_notime = gdf.drop(columns=['date'])\n",
    "# gdf_notime.to_file('result-08.shp')\n",
    "\n",
    "gdf_notime = gdf_utm.drop(columns=['date'])\n",
    "gdf_notime.to_file('snowex-20170208-utm.shp')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
