{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"> \n",
    "<img src='./img/header.png'/>\n",
    "</div>\n",
    "\n",
    "## [Global Ice Velocities](https://its-live.jpl.nasa.gov/)\n",
    "    \n",
    "The Inter-mission Time Series of Land Ice Velocity and Elevation (ITS_LIVE) project facilitates ice sheet, ice shelf and glacier research by providing a globally comprehensive and temporally dense multi-sensor record of land ice velocity and elevation with low latency.\n",
    "\n",
    "Scene-pair velocities generated from satellite optical and radar imagery.\n",
    "\n",
    "* Coverage: All land ice\n",
    "* Date range: 1985-present\n",
    "* Resolution: 240m\n",
    "* Scene-pair separation: 6 to 546 days\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "* If you want to query our API directly using  your own software here is the OpenApi endpoint https://staging.nsidc.org/apps/itslive-search/docs\n",
    "* For questions about this notebook and the dataset please contact users services at uso@nsidc.org\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1: Now let's render our UI and pick up an hemisphere, if you update the hemisphere you need to execute the cell again.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from itslive import itslive_ui\n",
    "ui = itslive_ui('global')\n",
    "ui.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2: We build the parameters to query the ITS_LIVE Search API, we get the time coverage for our selected area\n",
    "params = ui.build_params()\n",
    "print(f'current parameters: {params}')\n",
    "timeline = None\n",
    "if params is not None:\n",
    "    timeline = ui.update_coverages()\n",
    "    total =  sum(item['count'] for item in timeline)\n",
    "    print(f'Total data granules: {total:,}')\n",
    "timeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3: Now we are going to get the velocity pair urls, this does not download the files yet just their location\n",
    "urls = []\n",
    "params = ui.build_params()\n",
    "if params is not None:\n",
    "    urls = ui.get_granule_urls()\n",
    "    # Print the first 10 granule URLs\n",
    "    for url in urls[0:10]:\n",
    "        print(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering granules by year and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4: This will query AWS(where the granules are stored) so we know the total size of our first N granules\n",
    "# This may take some time, try reducing the selected area or constraining the other parameters to download a reasonable number of granules.\n",
    "url_list = [url['url'] for url in urls]\n",
    "# urls\n",
    "filtered_urls = ui.filter_urls(url_list, max_files_per_year=5, months=[3,4,5,6,7])\n",
    "\n",
    "# max_granules = 100\n",
    "# sizes = ui.calculate_file_sizes(filtered_urls, max_granules)\n",
    "# total_zise = round(sum(sizes)/1024,2)\n",
    "# print(f'Approx size to download for the first {max_granules:,} granules: {total_zise} MB')\n",
    "print(len(filtered_urls))\n",
    "filtered_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the data\n",
    "**Now that we have our list of data granules we can download them from AWS.**\n",
    "\n",
    "If this notebook is running inside AWS we could load the granules into a Dask cluster and reduce our processing times and costs.\n",
    "Let's get some coffee, some data requests are in the Gigabytes realm and may take a little while to be processed. \n",
    "Once that your status URL says is completed we can grab the HDF5 data file using the URL on the same response!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 This will download the first 50 velocity pairs\n",
    "files = ui.download_velocity_pairs(filtered_urls, start=0, end=50)\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "import pyproj\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from shapely.geometry import Polygon\n",
    "warnings.filterwarnings('ignore')\n",
    "centroid = ui.dc.last_draw['geometry']['coordinates']\n",
    "# coord = [-49.59321, 69.210579]\n",
    "#loads an array of xarray datasets from the nc files\n",
    "velocity_pairs = ui.load_velocity_pairs('data')\n",
    "\n",
    "centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocities = []\n",
    "mean_offset_meters = 1200\n",
    "for ds in velocity_pairs:\n",
    "    proj = str(int(ds.UTM_Projection.spatial_epsg))\n",
    "    selected_coord = ui.transform_coord('4326',proj, centroid[0], centroid[1])\n",
    "    projected_lon = round(selected_coord[0])\n",
    "    projected_lat = round(selected_coord[1])\n",
    "    mid_date = datetime.strptime(ds.img_pair_info.date_center,'%Y%m%d')\n",
    "    # We are going to calculate the mean value of the neighboring pixels(each is 240m) 10 x 10 window\n",
    "    mask_lon = (ds.x >= projected_lon - mean_offset_meters) & (ds.x <= projected_lon + mean_offset_meters)\n",
    "    mask_lat = (ds.y >= projected_lat - mean_offset_meters) & (ds.y <= projected_lat + mean_offset_meters)\n",
    "    v = ds.where(mask_lon & mask_lat , drop=True).v.mean(skipna=True)\n",
    "    # If we have a valid value we add it to the velocities array.\n",
    "    if not np.isnan(v):\n",
    "        velocities.append({'date': mid_date, 'mean_velocity': v.values.ravel()[0]})\n",
    "velocities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# order by date\n",
    "df = pd.DataFrame(velocities)\n",
    "df = df.sort_values(by='date').reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot(x ='date', y='mean_velocity', kind = 'bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "velocity_pairs[0].plot.scatter(x='x', y='y', hue='v')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
